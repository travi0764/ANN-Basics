{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49505dc9",
   "metadata": {},
   "source": [
    "## What is Perceptron ?\n",
    "\n",
    "    Perceptron is a single layer neural network and a multi-layer perceptron is called Neural Networks.\n",
    "\n",
    "    Perceptron is a linear classifier (binary). Also, it is used in supervised learning. It helps to classify the given input data. The perceptron consists of 4 parts.\n",
    "<img src = \"images/perceptron.png\" width = 700 height = 700>\n",
    "\n",
    "    1) Input values or One input layer\n",
    "    2) Weights and Bias\n",
    "    3) Net sum\n",
    "    4) Activation Function\n",
    "    \n",
    "    The Neural Networks work the same way as the perceptron. So, if you want to know how neural network works, learn how perceptron works.\n",
    "    \n",
    "    The perceptron works on these simple steps\n",
    "\n",
    "    a. All the inputs x are multiplied with their weights w. Letâ€™s call it k.\n",
    "    \n",
    "<img src = \"images/perceptron_inputs.png\" width = 500 height = 500>\n",
    "\n",
    "    b. Add all the multiplied values and call them Weighted Sum.\n",
    "    \n",
    "<img src = \"images/activation_function.gif\" width = 500 height = 500>\n",
    "\n",
    "    c. Apply that weighted sum to the correct Activation Function. For Example: Unit Step Activation Function.\n",
    "    \n",
    "<img src = \"images/activation_function.png\" width = 500 height = 500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad3c0c4",
   "metadata": {},
   "source": [
    "### Why do we need Weights and Bias?\n",
    "\n",
    "    Weights shows the strength of the particular node. A bias value allows you to shift the activation function curve up or down.\n",
    "\n",
    "<img src = \"https://miro.medium.com/max/1002/1*ztXU57QEETPHGXczHrSWSA.gif\" width = 500 height = 500>\n",
    "\n",
    "### Why do we need Activation Function?\n",
    "\n",
    "    In short, the activation functions are used to map the input between the required values like (0, 1) or (-1, 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fed0d0",
   "metadata": {},
   "source": [
    "### From iNeuron Notes\n",
    "\n",
    "``` \n",
    "    1. LTU and TLU is same as activation function. activation function is also known as LTU and TLU.\n",
    "    2. Weights are nothing but percentage of feature x{i} which will be passed to activation function. It is the amount of information from feature x{i} which is relevant for producing output y_hat.\n",
    "    3. we have to update weights according to errors we are getting. we have control over weights. the only thing we can control in whole process of producing output is weight, and weight is updated in each step after calculating errors.\n",
    "    4. z = w1x1+w2x2+w3x3 --> this is linear combination of weights and inputs, linear bcoz their power is 1, thats why z(threshold value) is also known as LTU or TLU.(Linear threshold unit).\n",
    "    5. bias is something extra usually errors which we provide with inputs to models, so that our model becomes more generalised and prune to errors. If a model is able to train with errors then it is a very good model.\n",
    "    6. bias is responsible for lateral(translational) movement of boundaries. and slope is responsible for rotational movement. So Bias is responsible for intercepts on x, and y axis and slopes are responsible for angles.\n",
    "```\n",
    "<img src = \"images/bias_significance.png\" width = 500 height = 500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6211f466",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
